{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a13be5d-9b17-4cd9-995d-68685b12af7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import csv\n",
    "import pandas as pd\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from transformers import BertTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29de413-f12d-4b9c-8cf7-a8fd93d397ac",
   "metadata": {},
   "source": [
    "#### Libraries and Modules:\n",
    "\n",
    "1. **re**: Regular expression library for pattern matching and manipulation.\n",
    "2. **nltk**: Natural Language Toolkit, a library for natural language processing.\n",
    "3. **csv**: Provides functionality for reading from and writing to CSV files.\n",
    "4. **pandas**: Data manipulation and analysis library.\n",
    "5. **wordnet**: Part of NLTK, a lexical database of English.\n",
    "6. **WordNetLemmatizer**: NLTK's lemmatizer for word lemmatization.\n",
    "7. **BertTokenizer**: Part of the Hugging Face `transformers` library, used for tokenization with BERT models.\n",
    "8. **word_tokenize**: NLTK's word tokenizer for breaking text into words.\n",
    "9. **stopwords**: NLTK's list of common stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52f5074d-87fd-4a5b-a041-8058af313825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the CSV files into pandas DataFrames\n",
    "physics_df = pd.read_csv('physics_papers.csv')\n",
    "medicine_df = pd.read_csv('medicine_papers.csv')\n",
    "cybersecurity_df = pd.read_csv('cybersecurity_papers.csv')\n",
    "\n",
    "# Add a new column 'category' to each DataFrame\n",
    "physics_df['category'] = 'Physics'\n",
    "medicine_df['category'] = 'Medicine'\n",
    "cybersecurity_df['category'] = 'Cybersecurity'\n",
    "\n",
    "# Ensure 'category' and 'Abstract' are the first two columns in each DataFrame\n",
    "physics_df = physics_df[['category', 'Abstract']]\n",
    "medicine_df = medicine_df[['category', 'Abstract']]\n",
    "cybersecurity_df = cybersecurity_df[['category', 'Abstract']]\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "combined_df = pd.concat([physics_df, medicine_df, cybersecurity_df], ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file\n",
    "combined_df.to_csv('input_file.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaccc1e-ded1-4e7c-a56d-7bedc3c34df3",
   "metadata": {},
   "source": [
    "#### Cell Explanation:\n",
    "\n",
    "#### Reading CSV Files:\n",
    "\n",
    "- **physics_df**, **medicine_df**, **cybersecurity_df**: Three pandas DataFrames created by reading data from CSV files ('physics_papers.csv', 'medicine_papers.csv', 'cybersecurity_papers.csv').\n",
    "\n",
    "#### Adding a New Column 'category':\n",
    "\n",
    "- A new column named 'category' is added to each DataFrame ('Physics' for physics_df, 'Medicine' for medicine_df, 'Cybersecurity' for cybersecurity_df).\n",
    "\n",
    "#### Reordering Columns:\n",
    "\n",
    "- The order of columns in each DataFrame is adjusted to ensure 'category' and 'Abstract' are the first two columns.\n",
    "\n",
    "#### Concatenating DataFrames:\n",
    "\n",
    "- The three DataFrames are concatenated into a single DataFrame, **combined_df**, using `pd.concat()`. The `ignore_index=True` argument resets the index of the resulting DataFrame.\n",
    "\n",
    "#### Saving Combined DataFrame:\n",
    "\n",
    "- The combined DataFrame is saved to a new CSV file named 'input_file.csv' using `to_csv()` with `index=False` to exclude the index column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67c0db91-c55e-4d08-8395-903e9fc308ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/baizid_alhamid/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/baizid_alhamid/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/baizid_alhamid/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/baizid_alhamid/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File processed and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Downloading necessary NLTK models for processing\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\"\"\"\n",
    "    Preprocessing the given text by applying several steps:\n",
    "    - Lowercasing all characters for uniformity.\n",
    "    - Removing special characters and digits to focus on words.\n",
    "    - Tokenizing the text into individual words.\n",
    "    - Removing common words or stop words that don't add much meaning.\n",
    "\"\"\"\n",
    "\n",
    "def get_pos_tag(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts.\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Initialize the WordNet Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Lemmatize each word using its POS (Part of Speech) tag\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, get_pos_tag(word)) for word in words]\n",
    "\n",
    "    # Join the lemmatized words back into a sentence\n",
    "    lemmatized_text = ' '.join(lemmatized_words)\n",
    "\n",
    "    return lemmatized_text\n",
    "\n",
    "\n",
    "def remove_special_characters_and_numbers(text):\n",
    "    # Define a regular expression pattern to match special characters and numbers\n",
    "    pattern = r'[^a-zA-Z\\s]'  # Keep only letters and whitespaces\n",
    "\n",
    "    # Apply the pattern to remove special characters and numbers\n",
    "    cleaned_text = re.sub(pattern, '', text)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def lowercase_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"Remove stopwords from the text.\"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "def clean_text(text):\n",
    "    # Identify and replace LaTeX expressions with placeholders\n",
    "    latex_expressions = []\n",
    "    def replace_latex(match):\n",
    "        latex_expressions.append(match.group())\n",
    "        return 'latexplaceholder'\n",
    "\n",
    "    # Define a pattern for identifying LaTeX expressions\n",
    "    latex_pattern = re.compile(r'(\\\\[(\\[{])[\\s\\S]+?\\\\[)\\]}]|(?<!\\\\)\\$\\$[\\s\\S]+?\\$\\$|\\$(?<!\\\\)[\\s\\S]+?\\$')\n",
    "\n",
    "    # Replace LaTeX expressions with placeholders\n",
    "    text_with_placeholders = latex_pattern.sub(replace_latex, text)\n",
    "\n",
    "    # Perform standard text cleaning on text_with_placeholders\n",
    "    # (e.g., lowercasing, removing special characters)\n",
    "    cleaned_text = remove_special_characters_and_numbers(text_with_placeholders)\n",
    "    cleaned_text = lowercase_text(cleaned_text)\n",
    "    cleaned_text = lemmatize_text(cleaned_text)\n",
    "    cleaned_text = remove_stopwords(cleaned_text)\n",
    "\n",
    "    # Replace placeholders with original LaTeX expressions\n",
    "    for latex_expr in latex_expressions:\n",
    "        cleaned_text = cleaned_text.replace('latexplaceholder', latex_expr, 1)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "def bert_tokenize(text, tokenizer):\n",
    "    # Tokenize the text using BERT tokenizer\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def clean_csv(input_file, output_file):\n",
    "    try:\n",
    "        # Load the BERT tokenizer\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        with open(input_file, 'r', newline='', encoding='utf-8') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            rows = list(reader)\n",
    "\n",
    "            # Apply cleaning and tokenization\n",
    "            bert_tokenized_data = [(row[0], bert_tokenize(clean_text(row[1]), tokenizer)) for row in rows]\n",
    "\n",
    "        with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerows(bert_tokenized_data)\n",
    "\n",
    "        print(\"File processed and saved successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "input_file = 'input_file.csv'\n",
    "output_file = 'output_file.csv'\n",
    "\n",
    "clean_csv(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11af441-fc9c-4c8d-b1bd-4225e8635b68",
   "metadata": {},
   "source": [
    "#### Cell Explanation:\n",
    "\n",
    "#### Downloading Necessary NLTK Models:\n",
    "\n",
    "- **nltk.download('stopwords')**: Downloads stopwords for later use.\n",
    "- **nltk.download('punkt')**: Downloads data required for tokenization.\n",
    "- **nltk.download('wordnet')**: Downloads WordNet, a lexical database for the English language.\n",
    "- **nltk.download('averaged_perceptron_tagger')**: Downloads data for POS tagging.\n",
    "\n",
    "#### Text Preprocessing Functions:\n",
    "\n",
    "1. **get_pos_tag(word)**:\n",
    "   - Maps POS (Part of Speech) tag to the first character lemmatize() accepts.\n",
    "\n",
    "2. **lemmatize_text(text)**:\n",
    "   - Tokenizes the text into words.\n",
    "   - Lemmatizes each word using its POS tag.\n",
    "   - Joins lemmatized words back into a sentence.\n",
    "\n",
    "3. **remove_special_characters_and_numbers(text)**:\n",
    "   - Removes special characters and numbers, keeping only letters and whitespaces.\n",
    "\n",
    "4. **lowercase_text(text)**:\n",
    "   - Converts the text to lowercase for uniformity.\n",
    "\n",
    "5. **remove_stopwords(text)**:\n",
    "   - Removes common English stopwords from the text.\n",
    "\n",
    "6. **clean_text(text)**:\n",
    "   - Replaces LaTeX expressions with placeholders.\n",
    "   - Performs standard text cleaning (lowercasing, removing special characters, lemmatization, removing stopwords).\n",
    "   - Restores original LaTeX expressions from placeholders.\n",
    "\n",
    "7. **bert_tokenize(text, tokenizer)**:\n",
    "   - Tokenizes the text using the BERT tokenizer.\n",
    "\n",
    "8. **clean_csv(input_file, output_file)**:\n",
    "   - Loads the BERT tokenizer.\n",
    "   - Reads a CSV file, applies cleaning and tokenization.\n",
    "   - Writes the processed data to a new CSV file.\n",
    "\n",
    "#### Script Execution:\n",
    "\n",
    "- The script reads data from 'input_file.csv', cleans and tokenizes the abstracts using the defined functions, and saves the results to 'output_file.csv'.\n",
    "- Any errors during execution are caught and printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae94722-c7f8-49bd-a081-6ca3ebf0cb20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
