## Analyzing Fully-Frozen, Half-Frozen, and Non-Frozen Layer Approaches in Fine-Tuning BERT Models for Scientific Abstract Classification ##

### Description:
This repository contains the code and report for a project aimed at investigating different fine-tuning approaches for BERT machine learning models in the context of classifying scientific abstracts. The project explores the effectiveness of fully-frozen, half-frozen, and non-frozen layer approaches in fine-tuning BERT for this specific task.

### Key Components:
- Jupyter Notebooks: Includes Python notebooks where the fine-tuning experiments were conducted using BERT.
- Report: A comprehensive report summarizing the project's objectives, methodologies, experimental setup, results, and   conclusions.

### Findings:
- Comparison of performance metrics (e.g., accuracy, F1 score) across different fine-tuning strategies.
- Insights into the effectiveness of freezing specific layers during fine-tuning.
- Recommendations for fine-tuning BERT models for scientific abstract classification tasks based on experimental results.

